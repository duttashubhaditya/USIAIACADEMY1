{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2021-11-19 17:14:53\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-19 17:14:54\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-19 17:14:55\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-19 17:14:56\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-19 17:14:57\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-19 17:14:58\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-19 17:14:59\n",
      "-------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init(\"/usr/local/spark\")\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import StreamingContext which is the main entry point for all streaming functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a SparkContext with two execution threads, and StreamingContext with batch interval of 1 second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(\"local[2]\", \"StreamingWordCount\")\n",
    "ssc = StreamingContext(sc, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this context, we can create a DStream that represents streaming data from a TCP source, specified as hostname (e.g. localhost) and port (e.g. 9999)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = ssc.socketTextStream(\"localhost\", 9999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lines DStream represents the stream of data that will be received from the data server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.streaming.dstream.DStream"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each record in this DStream is a line of text. Next, we need to split the lines by space into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = lines.flatMap(lambda line: line.split(\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This operation creates a new DStream named words. The elements of this DStream will be the individual words from the lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.streaming.dstream.TransformedDStream"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words DStream is further transformed to a DStream of (word, 1) pairs.\n",
    "And reduceByKey operation generates the count for each key i.e. word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = words.map(lambda word: (word, 1))\n",
    "wordCounts = pairs.reduceByKey(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use pprint (pretty print) function to print the counts generated every second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCounts.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the above code is executed the computations are only set up by SparkStreaming. They will be executed when SparkStreaming is started as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can terminate it by interrupting the kernel (sending Control+C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2021-11-19 16:57:41\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-19 16:57:42\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-19 16:57:43\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-19 16:57:44\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-19 16:57:45\n",
      "-------------------------------------------\n",
      "('service', 1)\n",
      "('What', 1)\n",
      "('is', 1)\n",
      "('are', 1)\n",
      "('looking', 1)\n",
      "('you', 1)\n",
      "('the', 1)\n",
      "('for?', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-19 16:57:46\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-19 16:57:47\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-19 16:57:48\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-19 16:57:49\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-19 16:57:50\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-19 16:57:51\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-19 16:57:52\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-19 16:57:53\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-19 16:57:54\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-19 16:57:55\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-19 16:57:56\n",
      "-------------------------------------------\n",
      "('good', 1)\n",
      "('are', 1)\n",
      "('medical', 1)\n",
      "('looking', 1)\n",
      "('service?', 1)\n",
      "('you', 1)\n",
      "('for', 1)\n",
      "('a', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-19 16:57:57\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-19 16:57:58\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-19 16:57:59\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-19 16:58:00\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-19 16:58:01\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-19 16:58:02\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-19 16:58:03\n",
      "-------------------------------------------\n",
      "('insured?', 1)\n",
      "('you', 1)\n",
      "('Are', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-19 16:58:04\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-19 16:58:05\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-19 16:58:06\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-19 16:58:07\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-19 16:58:08\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-19 16:58:09\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-19 16:58:10\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-19 16:58:11\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-19 16:58:12\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-19 16:58:13\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-19 16:58:14\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-19 16:58:15\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-19 16:58:16\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-19 16:58:17\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-19 16:58:18\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-19 16:58:19\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-19 16:58:20\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-19 16:58:21\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-19 16:58:22\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-19 16:58:23\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-19 16:58:24\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-19 16:58:25\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-19 16:58:26\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-19 16:58:27\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-19 16:58:28\n",
      "-------------------------------------------\n",
      "('do', 1)\n",
      "('visit', 1)\n",
      "('want', 1)\n",
      "('which', 1)\n",
      "('specialist', 1)\n",
      "('to', 1)\n",
      "('you', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-19 16:58:29\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-19 16:58:30\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-19 16:58:31\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-19 16:58:32\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-19 16:58:33\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-19 16:58:34\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-19 16:58:35\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-19 16:58:36\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-19 16:58:37\n",
      "-------------------------------------------\n",
      "('any', 1)\n",
      "('broadlevel', 1)\n",
      "('diagnosis?', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-19 16:58:38\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-19 16:58:39\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-19 16:58:40\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-19 16:58:41\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-19 16:58:42\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-19 16:58:43\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-19 16:58:44\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-19 16:58:45\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-19 16:58:46\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-19 16:58:47\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-19 16:58:48\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-19 16:58:49\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2021-11-19 16:58:50\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-19 16:58:51\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-19 16:58:52\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-19 16:58:53\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-19 16:58:54\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-19 16:58:55\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-19 16:58:56\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-19 16:58:57\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-19 16:58:58\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-6cfd70f7bded>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/streaming/context.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \"\"\"\n\u001b[1;32m    205\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTerminationOrTimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    573\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"/usr/local/spark\")\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import StreamingContext which is the main entry point for all streaming functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a SparkContext with two execution threads, and StreamingContext with batch interval of 1 second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(\"local[2]\", \"StreamingWordCount\")\n",
    "ssc = StreamingContext(sc, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this context, we can create a DStream that represents streaming data from a TCP source, specified as hostname (e.g. localhost) and port (e.g. 9999)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = ssc.socketTextStream(\"localhost\", 9999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lines DStream represents the stream of data that will be received from the data server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.streaming.dstream.DStream"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each record in this DStream is a line of text. Next, we need to split the lines by space into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines2 = sc.parallelize(['Hi how are you? Please comply. Please dont comply.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = lines.flatMap(lambda line: line.split(\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This operation creates a new DStream named words. The elements of this DStream will be the individual words from the lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.streaming.dstream.TransformedDStream"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.streaming.dstream.TransformedDStream at 0x7f0c7b6a3588>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words DStream is further transformed to a DStream of (word, 1) pairs.\n",
    "And reduceByKey operation generates the count for each key i.e. word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = words.map(lambda word: (word, 1))\n",
    "wordCounts = pairs.reduceByKey(lambda x, y: x + y)\n",
    "wordAvg = wordCounts.map(lambda x, y : (x * y)/(words.count())).reduceByKey(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use pprint (pretty print) function to print the Average words every second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordAvg.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the above code is executed the computations are only set up by SparkStreaming. They will be executed when SparkStreaming is started as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can terminate it by interrupting the kernel (sending Control+C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 148, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 408, in dump\n",
      "    self.save(obj)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 740, in save_tuple\n",
      "    save(element)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 255, in save_function\n",
      "    self.save_function_tuple(obj)\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 292, in save_function_tuple\n",
      "    save((code, closure, base_globals))\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 725, in save_tuple\n",
      "    save(element)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 770, in save_list\n",
      "    self._batch_appends(obj)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 794, in _batch_appends\n",
      "    save(x)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 255, in save_function\n",
      "    self.save_function_tuple(obj)\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 292, in save_function_tuple\n",
      "    save((code, closure, base_globals))\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 725, in save_tuple\n",
      "    save(element)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 770, in save_list\n",
      "    self._batch_appends(obj)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 794, in _batch_appends\n",
      "    save(x)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 255, in save_function\n",
      "    self.save_function_tuple(obj)\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 292, in save_function_tuple\n",
      "    save((code, closure, base_globals))\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 725, in save_tuple\n",
      "    save(element)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 770, in save_list\n",
      "    self._batch_appends(obj)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 794, in _batch_appends\n",
      "    save(x)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 255, in save_function\n",
      "    self.save_function_tuple(obj)\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 292, in save_function_tuple\n",
      "    save((code, closure, base_globals))\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 725, in save_tuple\n",
      "    save(element)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 770, in save_list\n",
      "    self._batch_appends(obj)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 797, in _batch_appends\n",
      "    save(tmp[0])\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 255, in save_function\n",
      "    self.save_function_tuple(obj)\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 292, in save_function_tuple\n",
      "    save((code, closure, base_globals))\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 725, in save_tuple\n",
      "    save(element)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 770, in save_list\n",
      "    self._batch_appends(obj)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 797, in _batch_appends\n",
      "    save(tmp[0])\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 249, in save_function\n",
      "    self.save_function_tuple(obj)\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 297, in save_function_tuple\n",
      "    save(f_globals)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 810, in save_dict\n",
      "    self._batch_setitems(obj.items())\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 841, in _batch_setitems\n",
      "    save(v)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 520, in save\n",
      "    self.save_reduce(obj=obj, *rv)\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 600, in save_reduce\n",
      "    save(state)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 810, in save_dict\n",
      "    self._batch_setitems(obj.items())\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 836, in _batch_setitems\n",
      "    save(v)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 520, in save\n",
      "    self.save_reduce(obj=obj, *rv)\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 600, in save_reduce\n",
      "    save(state)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 810, in save_dict\n",
      "    self._batch_setitems(obj.items())\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 836, in _batch_setitems\n",
      "    save(v)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 495, in save\n",
      "    rv = reduce(self.proto)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 332, in get_return_value\n",
      "    format(target_id, \".\", name, value))\n",
      "py4j.protocol.Py4JError: An error occurred while calling o26.__getstate__. Trace:\n",
      "py4j.Py4JException: Method __getstate__([]) does not exist\n",
      "\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)\n",
      "\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:274)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o24.awaitTermination.\n: org.apache.spark.SparkException: An exception was raised by Python:\nTraceback (most recent call last):\n  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 148, in dump\n    return Pickler.dump(self, obj)\n  File \"/usr/lib/python3.5/pickle.py\", line 408, in dump\n    self.save(obj)\n  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/usr/lib/python3.5/pickle.py\", line 740, in save_tuple\n    save(element)\n  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 255, in save_function\n    self.save_function_tuple(obj)\n  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 292, in save_function_tuple\n    save((code, closure, base_globals))\n  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/usr/lib/python3.5/pickle.py\", line 725, in save_tuple\n    save(element)\n  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/usr/lib/python3.5/pickle.py\", line 770, in save_list\n    self._batch_appends(obj)\n  File \"/usr/lib/python3.5/pickle.py\", line 794, in _batch_appends\n    save(x)\n  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 255, in save_function\n    self.save_function_tuple(obj)\n  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 292, in save_function_tuple\n    save((code, closure, base_globals))\n  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/usr/lib/python3.5/pickle.py\", line 725, in save_tuple\n    save(element)\n  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/usr/lib/python3.5/pickle.py\", line 770, in save_list\n    self._batch_appends(obj)\n  File \"/usr/lib/python3.5/pickle.py\", line 794, in _batch_appends\n    save(x)\n  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 255, in save_function\n    self.save_function_tuple(obj)\n  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 292, in save_function_tuple\n    save((code, closure, base_globals))\n  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/usr/lib/python3.5/pickle.py\", line 725, in save_tuple\n    save(element)\n  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/usr/lib/python3.5/pickle.py\", line 770, in save_list\n    self._batch_appends(obj)\n  File \"/usr/lib/python3.5/pickle.py\", line 794, in _batch_appends\n    save(x)\n  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 255, in save_function\n    self.save_function_tuple(obj)\n  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 292, in save_function_tuple\n    save((code, closure, base_globals))\n  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/usr/lib/python3.5/pickle.py\", line 725, in save_tuple\n    save(element)\n  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/usr/lib/python3.5/pickle.py\", line 770, in save_list\n    self._batch_appends(obj)\n  File \"/usr/lib/python3.5/pickle.py\", line 797, in _batch_appends\n    save(tmp[0])\n  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 255, in save_function\n    self.save_function_tuple(obj)\n  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 292, in save_function_tuple\n    save((code, closure, base_globals))\n  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/usr/lib/python3.5/pickle.py\", line 725, in save_tuple\n    save(element)\n  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/usr/lib/python3.5/pickle.py\", line 770, in save_list\n    self._batch_appends(obj)\n  File \"/usr/lib/python3.5/pickle.py\", line 797, in _batch_appends\n    save(tmp[0])\n  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 249, in save_function\n    self.save_function_tuple(obj)\n  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 297, in save_function_tuple\n    save(f_globals)\n  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/usr/lib/python3.5/pickle.py\", line 810, in save_dict\n    self._batch_setitems(obj.items())\n  File \"/usr/lib/python3.5/pickle.py\", line 841, in _batch_setitems\n    save(v)\n  File \"/usr/lib/python3.5/pickle.py\", line 520, in save\n    self.save_reduce(obj=obj, *rv)\n  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 600, in save_reduce\n    save(state)\n  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/usr/lib/python3.5/pickle.py\", line 810, in save_dict\n    self._batch_setitems(obj.items())\n  File \"/usr/lib/python3.5/pickle.py\", line 836, in _batch_setitems\n    save(v)\n  File \"/usr/lib/python3.5/pickle.py\", line 520, in save\n    self.save_reduce(obj=obj, *rv)\n  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 600, in save_reduce\n    save(state)\n  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/usr/lib/python3.5/pickle.py\", line 810, in save_dict\n    self._batch_setitems(obj.items())\n  File \"/usr/lib/python3.5/pickle.py\", line 836, in _batch_setitems\n    save(v)\n  File \"/usr/lib/python3.5/pickle.py\", line 495, in save\n    rv = reduce(self.proto)\n  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 332, in get_return_value\n    format(target_id, \".\", name, value))\npy4j.protocol.Py4JError: An error occurred while calling o26.__getstate__. Trace:\npy4j.Py4JException: Method __getstate__([]) does not exist\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326)\n\tat py4j.Gateway.invoke(Gateway.java:274)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 65, in call\n    r = self.func(t, *rdds)\n  File \"/usr/local/spark/python/pyspark/streaming/dstream.py\", line 631, in <lambda>\n    self.func = lambda t, rdd: func(t, prev_func(t, rdd))\n  File \"/usr/local/spark/python/pyspark/streaming/dstream.py\", line 295, in <lambda>\n    func = lambda t, rdd: oldfunc(rdd)\n  File \"/usr/local/spark/python/pyspark/streaming/dstream.py\", line 143, in func\n    return rdd.combineByKey(createCombiner, mergeValue, mergeCombiners, numPartitions)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1853, in combineByKey\n    shuffled = locally_combined.partitionBy(numPartitions, partitionFunc)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1790, in partitionBy\n    keyed._jrdd.rdd()).asJavaPairRDD()\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2462, in _jrdd\n    self._jrdd_deserializer, profiler)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2395, in _wrap_function\n    pickled_command, broadcast_vars, env, includes = _prepare_for_python_RDD(sc, command)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2381, in _prepare_for_python_RDD\n    pickled_command = ser.dumps(command)\n  File \"/usr/local/spark/python/pyspark/serializers.py\", line 464, in dumps\n    return cloudpickle.dumps(obj, 2)\n  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 704, in dumps\n    cp.dump(obj)\n  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 162, in dump\n    raise pickle.PicklingError(msg)\n_pickle.PicklingError: Could not serialize object: Py4JError: An error occurred while calling o26.__getstate__. Trace:\npy4j.Py4JException: Method __getstate__([]) does not exist\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326)\n\tat py4j.Gateway.invoke(Gateway.java:274)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\n\n\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n\tat org.apache.spark.streaming.api.python.PythonTransformedDStream.compute(PythonDStream.scala:246)\n\tat org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:342)\n\tat org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:342)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n\tat org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:341)\n\tat org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:341)\n\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)\n\tat org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:336)\n\tat org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:334)\n\tat scala.Option.orElse(Option.scala:289)\n\tat org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:331)\n\tat org.apache.spark.streaming.dstream.ForEachDStream.generateJob(ForEachDStream.scala:48)\n\tat org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:122)\n\tat org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:121)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n\tat scala.collection.AbstractTraversable.flatMap(Traversable.scala:104)\n\tat org.apache.spark.streaming.DStreamGraph.generateJobs(DStreamGraph.scala:121)\n\tat org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$3.apply(JobGenerator.scala:249)\n\tat org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$3.apply(JobGenerator.scala:247)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.streaming.scheduler.JobGenerator.generateJobs(JobGenerator.scala:247)\n\tat org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:183)\n\tat org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:89)\n\tat org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:88)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-6cfd70f7bded>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/streaming/context.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \"\"\"\n\u001b[1;32m    205\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTerminationOrTimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o24.awaitTermination.\n: org.apache.spark.SparkException: An exception was raised by Python:\nTraceback (most recent call last):\n  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 148, in dump\n    return Pickler.dump(self, obj)\n  File \"/usr/lib/python3.5/pickle.py\", line 408, in dump\n    self.save(obj)\n  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/usr/lib/python3.5/pickle.py\", line 740, in save_tuple\n    save(element)\n  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 255, in save_function\n    self.save_function_tuple(obj)\n  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 292, in save_function_tuple\n    save((code, closure, base_globals))\n  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/usr/lib/python3.5/pickle.py\", line 725, in save_tuple\n    save(element)\n  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/usr/lib/python3.5/pickle.py\", line 770, in save_list\n    self._batch_appends(obj)\n  File \"/usr/lib/python3.5/pickle.py\", line 794, in _batch_appends\n    save(x)\n  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 255, in save_function\n    self.save_function_tuple(obj)\n  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 292, in save_function_tuple\n    save((code, closure, base_globals))\n  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/usr/lib/python3.5/pickle.py\", line 725, in save_tuple\n    save(element)\n  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/usr/lib/python3.5/pickle.py\", line 770, in save_list\n    self._batch_appends(obj)\n  File \"/usr/lib/python3.5/pickle.py\", line 794, in _batch_appends\n    save(x)\n  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 255, in save_function\n    self.save_function_tuple(obj)\n  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 292, in save_function_tuple\n    save((code, closure, base_globals))\n  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/usr/lib/python3.5/pickle.py\", line 725, in save_tuple\n    save(element)\n  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/usr/lib/python3.5/pickle.py\", line 770, in save_list\n    self._batch_appends(obj)\n  File \"/usr/lib/python3.5/pickle.py\", line 794, in _batch_appends\n    save(x)\n  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 255, in save_function\n    self.save_function_tuple(obj)\n  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 292, in save_function_tuple\n    save((code, closure, base_globals))\n  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/usr/lib/python3.5/pickle.py\", line 725, in save_tuple\n    save(element)\n  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/usr/lib/python3.5/pickle.py\", line 770, in save_list\n    self._batch_appends(obj)\n  File \"/usr/lib/python3.5/pickle.py\", line 797, in _batch_appends\n    save(tmp[0])\n  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 255, in save_function\n    self.save_function_tuple(obj)\n  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 292, in save_function_tuple\n    save((code, closure, base_globals))\n  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/usr/lib/python3.5/pickle.py\", line 725, in save_tuple\n    save(element)\n  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/usr/lib/python3.5/pickle.py\", line 770, in save_list\n    self._batch_appends(obj)\n  File \"/usr/lib/python3.5/pickle.py\", line 797, in _batch_appends\n    save(tmp[0])\n  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 249, in save_function\n    self.save_function_tuple(obj)\n  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 297, in save_function_tuple\n    save(f_globals)\n  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/usr/lib/python3.5/pickle.py\", line 810, in save_dict\n    self._batch_setitems(obj.items())\n  File \"/usr/lib/python3.5/pickle.py\", line 841, in _batch_setitems\n    save(v)\n  File \"/usr/lib/python3.5/pickle.py\", line 520, in save\n    self.save_reduce(obj=obj, *rv)\n  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 600, in save_reduce\n    save(state)\n  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/usr/lib/python3.5/pickle.py\", line 810, in save_dict\n    self._batch_setitems(obj.items())\n  File \"/usr/lib/python3.5/pickle.py\", line 836, in _batch_setitems\n    save(v)\n  File \"/usr/lib/python3.5/pickle.py\", line 520, in save\n    self.save_reduce(obj=obj, *rv)\n  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 600, in save_reduce\n    save(state)\n  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/usr/lib/python3.5/pickle.py\", line 810, in save_dict\n    self._batch_setitems(obj.items())\n  File \"/usr/lib/python3.5/pickle.py\", line 836, in _batch_setitems\n    save(v)\n  File \"/usr/lib/python3.5/pickle.py\", line 495, in save\n    rv = reduce(self.proto)\n  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 332, in get_return_value\n    format(target_id, \".\", name, value))\npy4j.protocol.Py4JError: An error occurred while calling o26.__getstate__. Trace:\npy4j.Py4JException: Method __getstate__([]) does not exist\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326)\n\tat py4j.Gateway.invoke(Gateway.java:274)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 65, in call\n    r = self.func(t, *rdds)\n  File \"/usr/local/spark/python/pyspark/streaming/dstream.py\", line 631, in <lambda>\n    self.func = lambda t, rdd: func(t, prev_func(t, rdd))\n  File \"/usr/local/spark/python/pyspark/streaming/dstream.py\", line 295, in <lambda>\n    func = lambda t, rdd: oldfunc(rdd)\n  File \"/usr/local/spark/python/pyspark/streaming/dstream.py\", line 143, in func\n    return rdd.combineByKey(createCombiner, mergeValue, mergeCombiners, numPartitions)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1853, in combineByKey\n    shuffled = locally_combined.partitionBy(numPartitions, partitionFunc)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1790, in partitionBy\n    keyed._jrdd.rdd()).asJavaPairRDD()\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2462, in _jrdd\n    self._jrdd_deserializer, profiler)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2395, in _wrap_function\n    pickled_command, broadcast_vars, env, includes = _prepare_for_python_RDD(sc, command)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2381, in _prepare_for_python_RDD\n    pickled_command = ser.dumps(command)\n  File \"/usr/local/spark/python/pyspark/serializers.py\", line 464, in dumps\n    return cloudpickle.dumps(obj, 2)\n  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 704, in dumps\n    cp.dump(obj)\n  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 162, in dump\n    raise pickle.PicklingError(msg)\n_pickle.PicklingError: Could not serialize object: Py4JError: An error occurred while calling o26.__getstate__. Trace:\npy4j.Py4JException: Method __getstate__([]) does not exist\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326)\n\tat py4j.Gateway.invoke(Gateway.java:274)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\n\n\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n\tat org.apache.spark.streaming.api.python.PythonTransformedDStream.compute(PythonDStream.scala:246)\n\tat org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:342)\n\tat org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:342)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n\tat org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:341)\n\tat org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:341)\n\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)\n\tat org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:336)\n\tat org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:334)\n\tat scala.Option.orElse(Option.scala:289)\n\tat org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:331)\n\tat org.apache.spark.streaming.dstream.ForEachDStream.generateJob(ForEachDStream.scala:48)\n\tat org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:122)\n\tat org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:121)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n\tat scala.collection.AbstractTraversable.flatMap(Traversable.scala:104)\n\tat org.apache.spark.streaming.DStreamGraph.generateJobs(DStreamGraph.scala:121)\n\tat org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$3.apply(JobGenerator.scala:249)\n\tat org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$3.apply(JobGenerator.scala:247)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.streaming.scheduler.JobGenerator.generateJobs(JobGenerator.scala:247)\n\tat org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:183)\n\tat org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:89)\n\tat org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:88)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 148, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 408, in dump\n",
      "    self.save(obj)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 740, in save_tuple\n",
      "    save(element)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 255, in save_function\n",
      "    self.save_function_tuple(obj)\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 292, in save_function_tuple\n",
      "    save((code, closure, base_globals))\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 725, in save_tuple\n",
      "    save(element)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 770, in save_list\n",
      "    self._batch_appends(obj)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 794, in _batch_appends\n",
      "    save(x)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 255, in save_function\n",
      "    self.save_function_tuple(obj)\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 292, in save_function_tuple\n",
      "    save((code, closure, base_globals))\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 725, in save_tuple\n",
      "    save(element)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 770, in save_list\n",
      "    self._batch_appends(obj)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 794, in _batch_appends\n",
      "    save(x)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 255, in save_function\n",
      "    self.save_function_tuple(obj)\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 292, in save_function_tuple\n",
      "    save((code, closure, base_globals))\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 725, in save_tuple\n",
      "    save(element)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 770, in save_list\n",
      "    self._batch_appends(obj)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 794, in _batch_appends\n",
      "    save(x)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 255, in save_function\n",
      "    self.save_function_tuple(obj)\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 292, in save_function_tuple\n",
      "    save((code, closure, base_globals))\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 725, in save_tuple\n",
      "    save(element)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 770, in save_list\n",
      "    self._batch_appends(obj)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 797, in _batch_appends\n",
      "    save(tmp[0])\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 255, in save_function\n",
      "    self.save_function_tuple(obj)\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 292, in save_function_tuple\n",
      "    save((code, closure, base_globals))\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 725, in save_tuple\n",
      "    save(element)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 770, in save_list\n",
      "    self._batch_appends(obj)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 797, in _batch_appends\n",
      "    save(tmp[0])\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 249, in save_function\n",
      "    self.save_function_tuple(obj)\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 297, in save_function_tuple\n",
      "    save(f_globals)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 810, in save_dict\n",
      "    self._batch_setitems(obj.items())\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 841, in _batch_setitems\n",
      "    save(v)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 520, in save\n",
      "    self.save_reduce(obj=obj, *rv)\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 600, in save_reduce\n",
      "    save(state)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 810, in save_dict\n",
      "    self._batch_setitems(obj.items())\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 836, in _batch_setitems\n",
      "    save(v)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 520, in save\n",
      "    self.save_reduce(obj=obj, *rv)\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 600, in save_reduce\n",
      "    save(state)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 810, in save_dict\n",
      "    self._batch_setitems(obj.items())\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 836, in _batch_setitems\n",
      "    save(v)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 495, in save\n",
      "    rv = reduce(self.proto)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 332, in get_return_value\n",
      "    format(target_id, \".\", name, value))\n",
      "py4j.protocol.Py4JError: An error occurred while calling o26.__getstate__. Trace:\n",
      "py4j.Py4JException: Method __getstate__([]) does not exist\n",
      "\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)\n",
      "\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:274)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 148, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 408, in dump\n",
      "    self.save(obj)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 740, in save_tuple\n",
      "    save(element)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 255, in save_function\n",
      "    self.save_function_tuple(obj)\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 292, in save_function_tuple\n",
      "    save((code, closure, base_globals))\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 725, in save_tuple\n",
      "    save(element)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 770, in save_list\n",
      "    self._batch_appends(obj)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 794, in _batch_appends\n",
      "    save(x)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 255, in save_function\n",
      "    self.save_function_tuple(obj)\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 292, in save_function_tuple\n",
      "    save((code, closure, base_globals))\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 725, in save_tuple\n",
      "    save(element)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 770, in save_list\n",
      "    self._batch_appends(obj)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 794, in _batch_appends\n",
      "    save(x)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 255, in save_function\n",
      "    self.save_function_tuple(obj)\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 292, in save_function_tuple\n",
      "    save((code, closure, base_globals))\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 725, in save_tuple\n",
      "    save(element)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 770, in save_list\n",
      "    self._batch_appends(obj)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 794, in _batch_appends\n",
      "    save(x)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 255, in save_function\n",
      "    self.save_function_tuple(obj)\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 292, in save_function_tuple\n",
      "    save((code, closure, base_globals))\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 725, in save_tuple\n",
      "    save(element)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 770, in save_list\n",
      "    self._batch_appends(obj)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 797, in _batch_appends\n",
      "    save(tmp[0])\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 255, in save_function\n",
      "    self.save_function_tuple(obj)\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 292, in save_function_tuple\n",
      "    save((code, closure, base_globals))\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 725, in save_tuple\n",
      "    save(element)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 770, in save_list\n",
      "    self._batch_appends(obj)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 797, in _batch_appends\n",
      "    save(tmp[0])\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 249, in save_function\n",
      "    self.save_function_tuple(obj)\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 297, in save_function_tuple\n",
      "    save(f_globals)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 810, in save_dict\n",
      "    self._batch_setitems(obj.items())\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 841, in _batch_setitems\n",
      "    save(v)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 520, in save\n",
      "    self.save_reduce(obj=obj, *rv)\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 600, in save_reduce\n",
      "    save(state)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 810, in save_dict\n",
      "    self._batch_setitems(obj.items())\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 836, in _batch_setitems\n",
      "    save(v)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 520, in save\n",
      "    self.save_reduce(obj=obj, *rv)\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 600, in save_reduce\n",
      "    save(state)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 810, in save_dict\n",
      "    self._batch_setitems(obj.items())\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 836, in _batch_setitems\n",
      "    save(v)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 495, in save\n",
      "    rv = reduce(self.proto)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 332, in get_return_value\n",
      "    format(target_id, \".\", name, value))\n",
      "py4j.protocol.Py4JError: An error occurred while calling o26.__getstate__. Trace:\n",
      "py4j.Py4JException: Method __getstate__([]) does not exist\n",
      "\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)\n",
      "\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:274)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 148, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 408, in dump\n",
      "    self.save(obj)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 740, in save_tuple\n",
      "    save(element)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 255, in save_function\n",
      "    self.save_function_tuple(obj)\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 292, in save_function_tuple\n",
      "    save((code, closure, base_globals))\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 725, in save_tuple\n",
      "    save(element)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 770, in save_list\n",
      "    self._batch_appends(obj)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 794, in _batch_appends\n",
      "    save(x)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 255, in save_function\n",
      "    self.save_function_tuple(obj)\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 292, in save_function_tuple\n",
      "    save((code, closure, base_globals))\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 725, in save_tuple\n",
      "    save(element)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 770, in save_list\n",
      "    self._batch_appends(obj)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 794, in _batch_appends\n",
      "    save(x)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 255, in save_function\n",
      "    self.save_function_tuple(obj)\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 292, in save_function_tuple\n",
      "    save((code, closure, base_globals))\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 725, in save_tuple\n",
      "    save(element)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 770, in save_list\n",
      "    self._batch_appends(obj)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 794, in _batch_appends\n",
      "    save(x)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 255, in save_function\n",
      "    self.save_function_tuple(obj)\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 292, in save_function_tuple\n",
      "    save((code, closure, base_globals))\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 725, in save_tuple\n",
      "    save(element)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 770, in save_list\n",
      "    self._batch_appends(obj)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 797, in _batch_appends\n",
      "    save(tmp[0])\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 255, in save_function\n",
      "    self.save_function_tuple(obj)\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 292, in save_function_tuple\n",
      "    save((code, closure, base_globals))\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 725, in save_tuple\n",
      "    save(element)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 770, in save_list\n",
      "    self._batch_appends(obj)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 797, in _batch_appends\n",
      "    save(tmp[0])\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 249, in save_function\n",
      "    self.save_function_tuple(obj)\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 297, in save_function_tuple\n",
      "    save(f_globals)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 810, in save_dict\n",
      "    self._batch_setitems(obj.items())\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 841, in _batch_setitems\n",
      "    save(v)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 520, in save\n",
      "    self.save_reduce(obj=obj, *rv)\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 600, in save_reduce\n",
      "    save(state)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 810, in save_dict\n",
      "    self._batch_setitems(obj.items())\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 836, in _batch_setitems\n",
      "    save(v)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 520, in save\n",
      "    self.save_reduce(obj=obj, *rv)\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 600, in save_reduce\n",
      "    save(state)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 810, in save_dict\n",
      "    self._batch_setitems(obj.items())\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 836, in _batch_setitems\n",
      "    save(v)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 495, in save\n",
      "    rv = reduce(self.proto)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 332, in get_return_value\n",
      "    format(target_id, \".\", name, value))\n",
      "py4j.protocol.Py4JError: An error occurred while calling o26.__getstate__. Trace:\n",
      "py4j.Py4JException: Method __getstate__([]) does not exist\n",
      "\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)\n",
      "\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:274)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 148, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 408, in dump\n",
      "    self.save(obj)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 740, in save_tuple\n",
      "    save(element)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 255, in save_function\n",
      "    self.save_function_tuple(obj)\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 292, in save_function_tuple\n",
      "    save((code, closure, base_globals))\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 725, in save_tuple\n",
      "    save(element)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 770, in save_list\n",
      "    self._batch_appends(obj)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 794, in _batch_appends\n",
      "    save(x)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 255, in save_function\n",
      "    self.save_function_tuple(obj)\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 292, in save_function_tuple\n",
      "    save((code, closure, base_globals))\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 725, in save_tuple\n",
      "    save(element)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 770, in save_list\n",
      "    self._batch_appends(obj)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 794, in _batch_appends\n",
      "    save(x)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 255, in save_function\n",
      "    self.save_function_tuple(obj)\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 292, in save_function_tuple\n",
      "    save((code, closure, base_globals))\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 725, in save_tuple\n",
      "    save(element)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 770, in save_list\n",
      "    self._batch_appends(obj)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 794, in _batch_appends\n",
      "    save(x)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 255, in save_function\n",
      "    self.save_function_tuple(obj)\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 292, in save_function_tuple\n",
      "    save((code, closure, base_globals))\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 725, in save_tuple\n",
      "    save(element)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 770, in save_list\n",
      "    self._batch_appends(obj)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 797, in _batch_appends\n",
      "    save(tmp[0])\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 255, in save_function\n",
      "    self.save_function_tuple(obj)\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 292, in save_function_tuple\n",
      "    save((code, closure, base_globals))\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 725, in save_tuple\n",
      "    save(element)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 770, in save_list\n",
      "    self._batch_appends(obj)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 797, in _batch_appends\n",
      "    save(tmp[0])\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 249, in save_function\n",
      "    self.save_function_tuple(obj)\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 297, in save_function_tuple\n",
      "    save(f_globals)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 810, in save_dict\n",
      "    self._batch_setitems(obj.items())\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 841, in _batch_setitems\n",
      "    save(v)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 520, in save\n",
      "    self.save_reduce(obj=obj, *rv)\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 600, in save_reduce\n",
      "    save(state)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 810, in save_dict\n",
      "    self._batch_setitems(obj.items())\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 836, in _batch_setitems\n",
      "    save(v)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 520, in save\n",
      "    self.save_reduce(obj=obj, *rv)\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 600, in save_reduce\n",
      "    save(state)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 810, in save_dict\n",
      "    self._batch_setitems(obj.items())\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 836, in _batch_setitems\n",
      "    save(v)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 495, in save\n",
      "    rv = reduce(self.proto)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 332, in get_return_value\n",
      "    format(target_id, \".\", name, value))\n",
      "py4j.protocol.Py4JError: An error occurred while calling o26.__getstate__. Trace:\n",
      "py4j.Py4JException: Method __getstate__([]) does not exist\n",
      "\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)\n",
      "\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:274)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 148, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 408, in dump\n",
      "    self.save(obj)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 740, in save_tuple\n",
      "    save(element)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 255, in save_function\n",
      "    self.save_function_tuple(obj)\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 292, in save_function_tuple\n",
      "    save((code, closure, base_globals))\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 725, in save_tuple\n",
      "    save(element)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 770, in save_list\n",
      "    self._batch_appends(obj)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 794, in _batch_appends\n",
      "    save(x)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 255, in save_function\n",
      "    self.save_function_tuple(obj)\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 292, in save_function_tuple\n",
      "    save((code, closure, base_globals))\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 725, in save_tuple\n",
      "    save(element)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 770, in save_list\n",
      "    self._batch_appends(obj)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 794, in _batch_appends\n",
      "    save(x)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 255, in save_function\n",
      "    self.save_function_tuple(obj)\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 292, in save_function_tuple\n",
      "    save((code, closure, base_globals))\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 725, in save_tuple\n",
      "    save(element)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 770, in save_list\n",
      "    self._batch_appends(obj)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 794, in _batch_appends\n",
      "    save(x)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 255, in save_function\n",
      "    self.save_function_tuple(obj)\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 292, in save_function_tuple\n",
      "    save((code, closure, base_globals))\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 725, in save_tuple\n",
      "    save(element)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 770, in save_list\n",
      "    self._batch_appends(obj)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 797, in _batch_appends\n",
      "    save(tmp[0])\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 255, in save_function\n",
      "    self.save_function_tuple(obj)\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 292, in save_function_tuple\n",
      "    save((code, closure, base_globals))\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 725, in save_tuple\n",
      "    save(element)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 770, in save_list\n",
      "    self._batch_appends(obj)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 797, in _batch_appends\n",
      "    save(tmp[0])\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 249, in save_function\n",
      "    self.save_function_tuple(obj)\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 297, in save_function_tuple\n",
      "    save(f_globals)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 810, in save_dict\n",
      "    self._batch_setitems(obj.items())\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 841, in _batch_setitems\n",
      "    save(v)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 520, in save\n",
      "    self.save_reduce(obj=obj, *rv)\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 600, in save_reduce\n",
      "    save(state)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 810, in save_dict\n",
      "    self._batch_setitems(obj.items())\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 836, in _batch_setitems\n",
      "    save(v)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 520, in save\n",
      "    self.save_reduce(obj=obj, *rv)\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 600, in save_reduce\n",
      "    save(state)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 810, in save_dict\n",
      "    self._batch_setitems(obj.items())\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 836, in _batch_setitems\n",
      "    save(v)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 495, in save\n",
      "    rv = reduce(self.proto)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 332, in get_return_value\n",
      "    format(target_id, \".\", name, value))\n",
      "py4j.protocol.Py4JError: An error occurred while calling o26.__getstate__. Trace:\n",
      "py4j.Py4JException: Method __getstate__([]) does not exist\n",
      "\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)\n",
      "\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:274)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 148, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 408, in dump\n",
      "    self.save(obj)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 740, in save_tuple\n",
      "    save(element)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 255, in save_function\n",
      "    self.save_function_tuple(obj)\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 292, in save_function_tuple\n",
      "    save((code, closure, base_globals))\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 725, in save_tuple\n",
      "    save(element)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 770, in save_list\n",
      "    self._batch_appends(obj)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 794, in _batch_appends\n",
      "    save(x)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 255, in save_function\n",
      "    self.save_function_tuple(obj)\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 292, in save_function_tuple\n",
      "    save((code, closure, base_globals))\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 725, in save_tuple\n",
      "    save(element)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 770, in save_list\n",
      "    self._batch_appends(obj)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 794, in _batch_appends\n",
      "    save(x)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 255, in save_function\n",
      "    self.save_function_tuple(obj)\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 292, in save_function_tuple\n",
      "    save((code, closure, base_globals))\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 725, in save_tuple\n",
      "    save(element)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 770, in save_list\n",
      "    self._batch_appends(obj)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 794, in _batch_appends\n",
      "    save(x)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 255, in save_function\n",
      "    self.save_function_tuple(obj)\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 292, in save_function_tuple\n",
      "    save((code, closure, base_globals))\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 725, in save_tuple\n",
      "    save(element)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 770, in save_list\n",
      "    self._batch_appends(obj)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 797, in _batch_appends\n",
      "    save(tmp[0])\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 255, in save_function\n",
      "    self.save_function_tuple(obj)\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 292, in save_function_tuple\n",
      "    save((code, closure, base_globals))\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 725, in save_tuple\n",
      "    save(element)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 770, in save_list\n",
      "    self._batch_appends(obj)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 797, in _batch_appends\n",
      "    save(tmp[0])\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 249, in save_function\n",
      "    self.save_function_tuple(obj)\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 297, in save_function_tuple\n",
      "    save(f_globals)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 810, in save_dict\n",
      "    self._batch_setitems(obj.items())\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 841, in _batch_setitems\n",
      "    save(v)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 520, in save\n",
      "    self.save_reduce(obj=obj, *rv)\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 600, in save_reduce\n",
      "    save(state)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 810, in save_dict\n",
      "    self._batch_setitems(obj.items())\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 836, in _batch_setitems\n",
      "    save(v)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 520, in save\n",
      "    self.save_reduce(obj=obj, *rv)\n",
      "  File \"/usr/local/spark/python/pyspark/cloudpickle.py\", line 600, in save_reduce\n",
      "    save(state)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 810, in save_dict\n",
      "    self._batch_setitems(obj.items())\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 836, in _batch_setitems\n",
      "    save(v)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 495, in save\n",
      "    rv = reduce(self.proto)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 332, in get_return_value\n",
      "    format(target_id, \".\", name, value))\n",
      "py4j.protocol.Py4JError: An error occurred while calling o26.__getstate__. Trace:\n",
      "py4j.Py4JException: Method __getstate__([]) does not exist\n",
      "\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)\n",
      "\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:274)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

Make sure that all zookeeper instances are stopped.

sudo zookeeper-server-stop.sh $KAFKA_HOME/config/zookeeper.properties
==============================================================================================
zookeeper-server-start.sh $KAFKA_HOME/config/zookeeper.properties > /tmp/zkservinit.log 2>&1 &

kafka-server-start.sh $KAFKA_HOME/config/server.properties > /tmp/kfservinit.log 2>&1 &

kafka-topics.sh --list --zookeeper localhost:2181
kafka-topics.sh --zookeeper localhost:2181 --delete --topic first-topic

kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic first-topic

kafka-topics.sh --list --zookeeper localhost:2181

kafka-topics.sh --zookeeper localhost:2181 --describe --topic first-topic

kafka-console-producer.sh --broker-list localhost:9092 --topic first-topic
>This is a message
>This is another message

kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic first-topic --from-beginning

Consumer Group
--------------
kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 3 --topic first-topic
# Make sure topic is created with 3 partitions

kafka-console-producer.sh --broker-list localhost:9092 --topic first-topic
>This is message 1
>This is message 2
>This is message 3
>This is message 4
>This is message 5
>This is message 6

# Open terminal 2 and run a consumer
kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic first-topic --group my-first-application
# Open terminal 3 and run another consumer
kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic first-topic --group my-first-application
# Open terminal 4 and run another consumer
kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic first-topic --group my-first-application
# Now if we produce messages from producer terminal we will see that the messages are partitioned

# We can kill one of the consumers with Control+C and give more messages on producer
# We will notice that the messages are partitioned between existing consumers

Offset
------
kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic first-topic --group my-second-application --from-beginning
# The above will display all messages from the beginning
# Close this consumer. Run it again
kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic first-topic --group my-second-application --from-beginning
# We will see that beginning messages are not shown because offset is set to last message that was read
# Any new message will come through
# Close this consumer. Produce some messages
# We will now run the consumer
kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic first-topic --group my-second-application
# We are not giving the --from-beginning it will read from the last offset

Consumer Group Command
----------------------
kafka-consumer-groups.sh --bootstrap-server localhost:9092 --list
kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --group my-second-application
# While a consumer from this group is not running we will see the message as such.
# When one or two consumers from this group are running then we will be able to watch the consumers
# Lag can be show as well

# Reset the offset
kafka-consumer-groups.sh --bootstrap-server localhost:9092 --group my-second-application --reset-offsets --to-earliest --execute --topic first-topic

# We will now run the consumer
kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic first-topic --group my-second-application --from-beginning
# We will see that beginning messages are not shown because offset is set
# Close this consumer.

# Reset the offset back by 2 messages
kafka-consumer-groups.sh --bootstrap-server localhost:9092 --group my-second-application --reset-offsets --shift-by -2 --execute --topic first-topic

# We will now run the consumer
kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic first-topic --group my-second-application
# We are not giving the --from-beginning it will read from the last offset
# Close this consumer. 

=========================================================
kafka-server-stop.sh $KAFKA_HOME/config/server.properties
zookeeper-server-stop.sh $KAFKA_HOME/config/zookeeper.properties
================================================================
