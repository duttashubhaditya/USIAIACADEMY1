{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dwcYInYcEtMJ",
    "outputId": "2232de7d-aa99-4020-bda2-1e0e3e8270e4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\shubdutta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\shubdutta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\shubdutta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bZrBLHuEFjOq",
    "outputId": "046b229f-9574-4ff6-f887-8676e6378988"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['After', 'sleeping', 'for', 'four', 'hours', ',', 'he', 'decided', 'to', 'sleep', 'for', 'another', 'four']\n"
     ]
    }
   ],
   "source": [
    "text = \"After sleeping for four hours, he decided to sleep for another four\"\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "NbACaewYFeFg"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wNxWUmykE10D",
    "outputId": "cdcdc1ee-f1d2-444f-f9f3-a0d1ca56003a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'sample', 'sentence', ',', 'showing', 'off', 'the', 'stop', 'words', 'filtration', '.']\n",
      "['This', 'sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "example_sent = \"This is a sample sentence, showing off the stop words filtration.\"\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "word_tokens = word_tokenize(example_sent)\n",
    "\n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "\n",
    "filtered_sentence = []\n",
    "\n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "\n",
    "print(word_tokens)\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J7_SRfPdFW_t",
    "outputId": "ed6bab94-9313-48c6-d0b8-a23d561e966f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The United States of America (USA) or America, is a federal republic composed of 50 states\n",
      "the united states of america (usa) or america, is a federal republic composed of 50 states\n"
     ]
    }
   ],
   "source": [
    "text = \"The United States of America (USA) or America, is a federal republic composed of 50 states\"\n",
    "print(text)\n",
    "print(text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "EJSurALqFnmB"
   },
   "outputs": [],
   "source": [
    "def text_cleaner(text):\n",
    "    rules = [\n",
    "        {r'>\\s+': u'>'},  # remove spaces after a tag opens or closes\n",
    "        {r'\\s+': u' '},  # replace consecutive spaces\n",
    "        {r'\\s*<br\\s*/?>\\s*': u'\\n'},  # newline after a <br>\n",
    "        {r'</(div)\\s*>\\s*': u'\\n'},  # newline after </p> and </div> and <h1/>...\n",
    "        {r'</(p|h\\d)\\s*>\\s*': u'\\n\\n'},  # newline after </p> and </div> and <h1/>...\n",
    "        {r'<head>.*<\\s*(/head|body)[^>]*>': u''},  # remove <head> to </head>\n",
    "        {r'<a\\s+href=\"([^\"]+)\"[^>]*>.*</a>': r'\\1'},  # show links instead of texts\n",
    "        {r'[ \\t]*<[^<]*?/?>': u''},  # remove remaining tags\n",
    "        {r'^\\s+': u''}  # remove spaces at the beginning\n",
    "    ]\n",
    "    for rule in rules:\n",
    "      for (k, v) in rule.items():\n",
    "          regex = re.compile(k)\n",
    "          text = regex.sub(v, text)\n",
    "          text = text.rstrip()\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FvSLo2asFtU5"
   },
   "outputs": [],
   "source": [
    "# from autocorrect import spell\n",
    "\n",
    "# print(spell('caaaar'))\n",
    "# print(spell(u'mussage'))\n",
    "# print(spell(u'survice'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ogBMG_tfF1b_",
    "outputId": "07b2fe3e-1198-404d-8029-2401d1e5c6c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\n",
      "python\n",
      "python\n",
      "python\n",
      "pythonli\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "example_words = [\"python\",\"pythoner\",\"pythoning\",\"pythoned\",\"pythonly\"]\n",
    "\n",
    "for w in example_words:\n",
    "  print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AH7AdL5FF_Ty",
    "outputId": "df4528d2-cb34-4c6e-a1c5-ffd8b0d3bfad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cow\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(lemmatizer.lemmatize(\"cows\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The\n",
      "striped\n",
      "bat\n",
      "are\n",
      "hanging\n",
      "on\n",
      "their\n",
      "foot\n",
      "for\n",
      "bested\n"
     ]
    }
   ],
   "source": [
    "tok = ['The', 'striped', 'bats', 'are', 'hanging', 'on', 'their', 'feet', 'for', 'bested']\n",
    "for t in tok:\n",
    "    print(lemmatizer.lemmatize(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "IcLtGTv4GFBE"
   },
   "outputs": [],
   "source": [
    "sentences = ['it is a good movie', 'it is not a good movie', 'i did not like it']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_list = []\n",
    "token_list = []\n",
    "for sen in sentences:\n",
    "    tokens = word_tokenize(sen)\n",
    "    token_list.append(tokens)\n",
    "    for tok in tokens:\n",
    "        if tok not in unique_list:\n",
    "              unique_list.append(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['it', 'is', 'a', 'good', 'movie', 'not', 'i', 'did', 'like'] [['it', 'is', 'a', 'good', 'movie'], ['it', 'is', 'not', 'a', 'good', 'movie'], ['i', 'did', 'not', 'like', 'it']]\n"
     ]
    }
   ],
   "source": [
    "print(unique_list, token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['it', 'is', 'a', 'good', 'movie'],\n",
       " ['it', 'is', 'not', 'a', 'good', 'movie'],\n",
       " ['i', 'did', 'not', 'like', 'it']]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "gram_1 = []\n",
    "\n",
    "for token in token_list:\n",
    "    # print(token)\n",
    "    temp_list = []\n",
    "    for un in unique_list:\n",
    "          if un in token:\n",
    "        # print(\"1\")\n",
    "            temp_list.append(1)\n",
    "          else:\n",
    "        # print(\"0\")\n",
    "            temp_list.append(0)\n",
    "    print(temp_list)\n",
    "    gram_1.append(temp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(gram_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "text1 = [\"it is a good movie\"]\n",
    "text2 = [\"it is not a good movie\"]\n",
    "text3 = [\"i did not like it\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'it': 2, 'is': 1, 'good': 0, 'movie': 3}\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "newvector = vectorizer.transform(text2)\n",
    "\n",
    "print(newvector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "newvector1 = vectorizer.transform(text3)\n",
    "print(newvector1.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"it is a good movie\",\"it is not a good movie\",\"i did not like it\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer()"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.fit(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.69314718 1.28768207 1.28768207 1.         1.69314718 1.28768207\n",
      " 1.28768207]\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.idf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'it': 3, 'is': 2, 'good': 1, 'movie': 5, 'not': 6, 'did': 0, 'like': 4}\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = vectorizer.transform([text[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 6)\n",
      "      first  first document  the first  the first document   this is  \\\n",
      "0  0.408248        0.408248   0.408248            0.408248  0.408248   \n",
      "1  0.000000        0.000000   0.000000            0.000000  0.000000   \n",
      "2  0.000000        0.000000   0.000000            0.000000  0.707107   \n",
      "3  0.500000        0.500000   0.500000            0.500000  0.000000   \n",
      "\n",
      "   this is the  \n",
      "0     0.408248  \n",
      "1     0.000000  \n",
      "2     0.707107  \n",
      "3     0.000000  \n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=0.5, ngram_range=(1,3),\n",
    "                                 min_df=2,\n",
    "                                 )\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "df = pd.DataFrame(\n",
    "    X.todense(),\n",
    "    columns=vectorizer.get_feature_names()\n",
    ")\n",
    "\n",
    "\n",
    "print(X.shape)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "orpus = [\n",
    "    'Data mining is one of the important research in the domain of Artificial Intelligence.',\n",
    "    'Artificial Intelligence is a subject in Computer Science.',\n",
    "    'Now a days, Artifical Intelligence is most commonly used in machine learning',\n",
    "    'This particulary statement speaks about Aritifical Intelligence',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        and  and this  and this is  document is  document is the  \\\n",
      "0  0.000000  0.000000     0.000000     0.000000         0.000000   \n",
      "1  0.000000  0.000000     0.000000     0.333333         0.333333   \n",
      "2  0.333333  0.333333     0.333333     0.000000         0.000000   \n",
      "3  0.000000  0.000000     0.000000     0.000000         0.000000   \n",
      "\n",
      "   is the first  is the second  is the third  is this  is this the  ...  \\\n",
      "0           1.0       0.000000      0.000000      0.0          0.0  ...   \n",
      "1           0.0       0.333333      0.000000      0.0          0.0  ...   \n",
      "2           0.0       0.000000      0.333333      0.0          0.0  ...   \n",
      "3           0.0       0.000000      0.000000      0.5          0.5  ...   \n",
      "\n",
      "   the second  the second document  the third  the third one     third  \\\n",
      "0    0.000000             0.000000   0.000000       0.000000  0.000000   \n",
      "1    0.333333             0.333333   0.000000       0.000000  0.000000   \n",
      "2    0.000000             0.000000   0.333333       0.333333  0.333333   \n",
      "3    0.000000             0.000000   0.000000       0.000000  0.000000   \n",
      "\n",
      "   third one  this document  this document is  this the  this the first  \n",
      "0   0.000000       0.000000          0.000000       0.0             0.0  \n",
      "1   0.000000       0.333333          0.333333       0.0             0.0  \n",
      "2   0.333333       0.000000          0.000000       0.0             0.0  \n",
      "3   0.000000       0.000000          0.000000       0.5             0.5  \n",
      "\n",
      "[4 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=0.4, ngram_range=(1,3),\n",
    "                                 min_df=1,\n",
    "                                 )\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "df = pd.DataFrame(\n",
    "    X.todense(),\n",
    "    columns=vectorizer.get_feature_names()\n",
    ")\n",
    "print(df)\n",
    "df.to_csv('sample.csv')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "text_analysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
