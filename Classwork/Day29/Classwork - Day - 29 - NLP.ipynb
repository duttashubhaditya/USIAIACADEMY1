{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "622b4df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8ff4a866",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"After sleeping for four hours, he decided to sleep for another four!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b1334839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['After',\n",
       " 'sleeping',\n",
       " 'for',\n",
       " 'four',\n",
       " 'hours,',\n",
       " 'he',\n",
       " 'decided',\n",
       " 'to',\n",
       " 'sleep',\n",
       " 'for',\n",
       " 'another',\n",
       " 'four!']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list = text.split(' ')\n",
    "word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fe36b7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex => Regular Expression | Compiler Design | Theory of computation\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9b8fad08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'After sleeping for four hours he decided to sleep for another four '"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = re.sub(r\"[^a-zA-Z0-9]+\", ' ', text)\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8130a739",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = ['a','and', 'the', 'an', 'he', 'she', 'for']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a2819cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n"
     ]
    }
   ],
   "source": [
    "for word in word_list:\n",
    "    if word not in stop_words:\n",
    "        print(\"yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f5234058",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['After',\n",
       " 'sleeping',\n",
       " 'four',\n",
       " 'hours,',\n",
       " 'decided',\n",
       " 'to',\n",
       " 'sleep',\n",
       " 'another',\n",
       " 'four!']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refined_wlist = []\n",
    "for word in word_list:\n",
    "    if word not in stop_words:\n",
    "        refined_wlist.append(word)\n",
    "        \n",
    "refined_wlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "67ec6e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fcfb6f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "stems = ['ing','s', 'ed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9352566a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sleeping\n",
      "sleeping\n",
      "hours,\n",
      "decided\n",
      "sleep\n"
     ]
    }
   ],
   "source": [
    "for word in refined_wlist:\n",
    "    for st in stems:\n",
    "        if st in word:\n",
    "            print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d17a87cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_list = []\n",
    "for word in refined_wlist:\n",
    "    flag = 0\n",
    "    for st in stems:\n",
    "        if word.find(st)>0:\n",
    "            new_list.append(word.replace(st,''))\n",
    "            flag = 1\n",
    "            break\n",
    "    if flag == 0:\n",
    "        new_list.append(word)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f2e2aa89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['After', 'sleep', 'four', 'hour,', 'decid', 'to', 'sleep', 'another', 'four!']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "93f3e021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['after', 'sleep', 'four', 'hour,', 'decid', 'to', 'sleep', 'another', 'four!']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "world_list_l = []\n",
    "\n",
    "for word in new_list:\n",
    "    world_list_l.append(word.lower())\n",
    "\n",
    "world_list_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "daf8134a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cow\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(lemmatizer.lemmatize(\"cows\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "40f52381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The\n",
      "striped\n",
      "bat\n",
      "are\n",
      "hanging\n",
      "on\n",
      "their\n",
      "foot\n",
      "for\n",
      "bested\n"
     ]
    }
   ],
   "source": [
    "tok = ['The', 'striped', 'bats', 'are', 'hanging', 'on', 'their', 'feet', 'for', 'bested']\n",
    "for t in tok:\n",
    "    print(lemmatizer.lemmatize(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "85786857",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['it is a good movie', 'it is not a good movie', 'i did not like it']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "84a54d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_list = []\n",
    "token_list = []\n",
    "for sen in sentences:\n",
    "    tokens = word_tokenize(sen)\n",
    "    token_list.append(tokens)\n",
    "    for tok in tokens:\n",
    "        if tok not in unique_list:\n",
    "              unique_list.append(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d36540a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['it', 'is', 'a', 'good', 'movie', 'not', 'i', 'did', 'like'] [['it', 'is', 'a', 'good', 'movie'], ['it', 'is', 'not', 'a', 'good', 'movie'], ['i', 'did', 'not', 'like', 'it']]\n"
     ]
    }
   ],
   "source": [
    "print(unique_list, token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "72218bde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['it', 'is', 'a', 'good', 'movie'],\n",
       " ['it', 'is', 'not', 'a', 'good', 'movie'],\n",
       " ['i', 'did', 'not', 'like', 'it']]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2649bdea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "gram_1 = []\n",
    "\n",
    "for token in token_list:\n",
    "    # print(token)\n",
    "    temp_list = []\n",
    "    for un in unique_list:\n",
    "          if un in token:\n",
    "        # print(\"1\")\n",
    "            temp_list.append(1)\n",
    "          else:\n",
    "        # print(\"0\")\n",
    "            temp_list.append(0)\n",
    "    print(temp_list)\n",
    "    gram_1.append(temp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b0d25e62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array(gram_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ead11f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "text1 = [\"it is a good movie\"]\n",
    "text2 = [\"it is not a good movie\"]\n",
    "text3 = [\"i did not like it\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "364296ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e146d17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'it': 2, 'is': 1, 'good': 0, 'movie': 3}\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0eafd8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "newvector = vectorizer.transform(text2)\n",
    "\n",
    "print(newvector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "41f06674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "newvector1 = vectorizer.transform(text3)\n",
    "print(newvector1.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9f78a9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7c1e4bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"it is a good movie\",\"it is not a good movie\",\"i did not like it\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "90e50d43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer()"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.fit(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a6bc642e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.69314718 1.28768207 1.28768207 1.         1.69314718 1.28768207\n",
      " 1.28768207]\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.idf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e824627d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'it': 3, 'is': 2, 'good': 1, 'movie': 5, 'not': 6, 'did': 0, 'like': 4}\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f0213f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = vectorizer.transform([text[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9f755fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5844829  0.         0.         0.34520502 0.5844829  0.\n",
      "  0.44451431]]\n"
     ]
    }
   ],
   "source": [
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f53f674b",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0579426f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b1e622bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 6)\n",
      "      first  first document  the first  the first document   this is  \\\n",
      "0  0.408248        0.408248   0.408248            0.408248  0.408248   \n",
      "1  0.000000        0.000000   0.000000            0.000000  0.000000   \n",
      "2  0.000000        0.000000   0.000000            0.000000  0.707107   \n",
      "3  0.500000        0.500000   0.500000            0.500000  0.000000   \n",
      "\n",
      "   this is the  \n",
      "0     0.408248  \n",
      "1     0.000000  \n",
      "2     0.707107  \n",
      "3     0.000000  \n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=0.5, ngram_range=(1,3),\n",
    "                                 min_df=2,\n",
    "                                 )\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "df = pd.DataFrame(\n",
    "    X.todense(),\n",
    "    columns=vectorizer.get_feature_names()\n",
    ")\n",
    "\n",
    "\n",
    "print(X.shape)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "768016e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'Data mining is one of the important research in the domain of Artificial Intelligence.',\n",
    "    'Artificial Intelligence is a subject in Computer Science.',\n",
    "    'Now a days, Artifical Intelligence is most commonly used in machine learning',\n",
    "    'This particulary statement speaks about Aritifical Intelligence',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "01307835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      about  about aritifical  about aritifical intelligence  aritifical  \\\n",
      "0  0.000000          0.000000                       0.000000    0.000000   \n",
      "1  0.000000          0.000000                       0.000000    0.000000   \n",
      "2  0.000000          0.000000                       0.000000    0.000000   \n",
      "3  0.242536          0.242536                       0.242536    0.242536   \n",
      "\n",
      "   aritifical intelligence  artifical  artifical intelligence  \\\n",
      "0                 0.000000   0.000000                0.000000   \n",
      "1                 0.000000   0.000000                0.000000   \n",
      "2                 0.000000   0.196116                0.196116   \n",
      "3                 0.242536   0.000000                0.000000   \n",
      "\n",
      "   artifical intelligence is  artificial intelligence is  commonly  ...  \\\n",
      "0                   0.000000                    0.000000  0.000000  ...   \n",
      "1                   0.000000                    0.288675  0.000000  ...   \n",
      "2                   0.196116                    0.000000  0.196116  ...   \n",
      "3                   0.000000                    0.000000  0.000000  ...   \n",
      "\n",
      "   the domain  the domain of  the important  the important research      this  \\\n",
      "0    0.162221       0.162221       0.162221                0.162221  0.000000   \n",
      "1    0.000000       0.000000       0.000000                0.000000  0.000000   \n",
      "2    0.000000       0.000000       0.000000                0.000000  0.000000   \n",
      "3    0.000000       0.000000       0.000000                0.000000  0.242536   \n",
      "\n",
      "   this particulary  this particulary statement      used   used in  \\\n",
      "0          0.000000                    0.000000  0.000000  0.000000   \n",
      "1          0.000000                    0.000000  0.000000  0.000000   \n",
      "2          0.000000                    0.000000  0.196116  0.196116   \n",
      "3          0.242536                    0.242536  0.000000  0.000000   \n",
      "\n",
      "   used in machine  \n",
      "0         0.000000  \n",
      "1         0.000000  \n",
      "2         0.196116  \n",
      "3         0.000000  \n",
      "\n",
      "[4 rows x 87 columns]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=0.4, ngram_range=(1,3),\n",
    "                                 min_df=1,\n",
    "                                 )\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "df = pd.DataFrame(\n",
    "    X.todense(),\n",
    "    columns=vectorizer.get_feature_names()\n",
    ")\n",
    "print(df)\n",
    "df.to_csv('sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8d63ee95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   about aritifical  aritifical intelligence  artifical intelligence  \\\n",
      "0          0.000000                 0.000000                0.000000   \n",
      "1          0.000000                 0.000000                0.000000   \n",
      "2          0.000000                 0.000000                0.322386   \n",
      "3          0.408248                 0.408248                0.000000   \n",
      "\n",
      "   artificial intelligence  commonly used  computer science  data mining  \\\n",
      "0                 0.221920       0.000000          0.000000     0.281477   \n",
      "1                 0.344315       0.000000          0.436719     0.000000   \n",
      "2                 0.000000       0.322386          0.000000     0.000000   \n",
      "3                 0.000000       0.000000          0.000000     0.000000   \n",
      "\n",
      "   days artifical  domain of  important research  ...    one of  \\\n",
      "0        0.000000   0.281477            0.281477  ...  0.281477   \n",
      "1        0.000000   0.000000            0.000000  ...  0.000000   \n",
      "2        0.322386   0.000000            0.000000  ...  0.000000   \n",
      "3        0.000000   0.000000            0.000000  ...  0.000000   \n",
      "\n",
      "   particulary statement  research in  speaks about  statement speaks  \\\n",
      "0               0.000000     0.281477      0.000000          0.000000   \n",
      "1               0.000000     0.000000      0.000000          0.000000   \n",
      "2               0.000000     0.000000      0.000000          0.000000   \n",
      "3               0.408248     0.000000      0.408248          0.408248   \n",
      "\n",
      "   subject in  the domain  the important  this particulary   used in  \n",
      "0    0.000000    0.281477       0.281477          0.000000  0.000000  \n",
      "1    0.436719    0.000000       0.000000          0.000000  0.000000  \n",
      "2    0.000000    0.000000       0.000000          0.000000  0.322386  \n",
      "3    0.000000    0.000000       0.000000          0.408248  0.000000  \n",
      "\n",
      "[4 rows x 33 columns]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=0.5, ngram_range=(2,2),\n",
    "                                 min_df=1,\n",
    "                                 )\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "df = pd.DataFrame(\n",
    "    X.todense(),\n",
    "    columns=vectorizer.get_feature_names()\n",
    ")\n",
    "print(df)\n",
    "df.to_csv('sample1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "88c50e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv('data_train.csv', encoding='utf-8')\n",
    "data_test = pd.read_csv('data_test.csv', encoding='utf-8')\n",
    "\n",
    "X_train = data_train.Text\n",
    "X_test = data_test.Text\n",
    "\n",
    "y_train = data_train.Emotion\n",
    "y_test = data_test.Emotion\n",
    "\n",
    "data = data_train.append(data_test, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "be5beffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['joy', 'fear', 'anger', 'sadness', 'neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b0340eea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>There are tons of other paintings that I thin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sadness</td>\n",
       "      <td>Yet the dog had grown old and less capable , a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fear</td>\n",
       "      <td>When I get into the tube or the train without ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fear</td>\n",
       "      <td>This last may be a source of considerable disq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>anger</td>\n",
       "      <td>She disliked the intimacy he showed towards so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11322</th>\n",
       "      <td>sadness</td>\n",
       "      <td>My sweetheart left me, or rather we decided to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11323</th>\n",
       "      <td>sadness</td>\n",
       "      <td>Well , it's too bad that we like different kin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11324</th>\n",
       "      <td>neutral</td>\n",
       "      <td>It sure is .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11325</th>\n",
       "      <td>sadness</td>\n",
       "      <td>He ’ s got laid off again . I do feel sorry fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11326</th>\n",
       "      <td>anger</td>\n",
       "      <td>When stupid people push me during rush time in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11327 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Emotion                                               Text\n",
       "0      neutral   There are tons of other paintings that I thin...\n",
       "1      sadness  Yet the dog had grown old and less capable , a...\n",
       "2         fear  When I get into the tube or the train without ...\n",
       "3         fear  This last may be a source of considerable disq...\n",
       "4        anger  She disliked the intimacy he showed towards so...\n",
       "...        ...                                                ...\n",
       "11322  sadness  My sweetheart left me, or rather we decided to...\n",
       "11323  sadness  Well , it's too bad that we like different kin...\n",
       "11324  neutral                                      It sure is . \n",
       "11325  sadness  He ’ s got laid off again . I do feel sorry fo...\n",
       "11326    anger  When stupid people push me during rush time in...\n",
       "\n",
       "[11327 rows x 2 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f52fefcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joy        2326\n",
      "sadness    2317\n",
      "anger      2259\n",
      "neutral    2254\n",
      "fear       2171\n",
      "Name: Emotion, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data.Emotion.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4ab0774b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['neutral', 'sadness', 'fear', 'anger', 'joy'], dtype=object)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Emotion.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2d16ab84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fear</td>\n",
       "      <td>When I get into the tube or the train without ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fear</td>\n",
       "      <td>This last may be a source of considerable disq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>fear</td>\n",
       "      <td>I had gone to the hospital for my research and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>fear</td>\n",
       "      <td>When I have to leave the baby in the carriage ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>fear</td>\n",
       "      <td>Infiltration in our lives. The illusion of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>fear</td>\n",
       "      <td>When biking and I felt very bad (problems with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>fear</td>\n",
       "      <td>Whenever I am alone in a dark room, walk alone...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>fear</td>\n",
       "      <td>When I got in jungle where there was a great n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>fear</td>\n",
       "      <td>When I was involved in a car accident last Nov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>fear</td>\n",
       "      <td>Saw a classmate knocked down by a motorcycle.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Emotion                                               Text\n",
       "2     fear  When I get into the tube or the train without ...\n",
       "3     fear  This last may be a source of considerable disq...\n",
       "10    fear  I had gone to the hospital for my research and...\n",
       "17    fear  When I have to leave the baby in the carriage ...\n",
       "20    fear     Infiltration in our lives. The illusion of ...\n",
       "34    fear  When biking and I felt very bad (problems with...\n",
       "35    fear  Whenever I am alone in a dark room, walk alone...\n",
       "37    fear  When I got in jungle where there was a great n...\n",
       "49    fear  When I was involved in a car accident last Nov...\n",
       "53    fear      Saw a classmate knocked down by a motorcycle."
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fear_df = data[data['Emotion']=='fear'].head(10)\n",
    "fear_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ba0cd6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     accept  accident     after     alone        am        an       and  \\\n",
      "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.112720   \n",
      "2  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.112480   \n",
      "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.148407   \n",
      "4  0.296792  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "5  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.392984   \n",
      "6  0.000000  0.000000  0.000000  0.467241  0.155747  0.155747  0.000000   \n",
      "7  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "8  0.000000  0.142548  0.142548  0.000000  0.000000  0.000000  0.169296   \n",
      "9  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "\n",
      "         as        at      baby  ...       was        we  whenever     where  \\\n",
      "0  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
      "1  0.161366  0.161366  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
      "2  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
      "3  0.000000  0.000000  0.499837  ...  0.000000  0.000000  0.000000  0.000000   \n",
      "4  0.000000  0.000000  0.000000  ...  0.000000  0.296792  0.000000  0.000000   \n",
      "5  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
      "6  0.132399  0.132399  0.000000  ...  0.115834  0.000000  0.155747  0.000000   \n",
      "7  0.000000  0.000000  0.000000  ...  0.274309  0.000000  0.000000  0.368829   \n",
      "8  0.000000  0.000000  0.000000  ...  0.212035  0.000000  0.000000  0.000000   \n",
      "9  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
      "\n",
      "      which      will      with   without     would      year  \n",
      "0  0.000000  0.000000  0.000000  0.344106  0.000000  0.000000  \n",
      "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "2  0.000000  0.000000  0.000000  0.000000  0.189417  0.000000  \n",
      "3  0.000000  0.249918  0.000000  0.000000  0.000000  0.000000  \n",
      "4  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "5  0.000000  0.000000  0.330895  0.000000  0.000000  0.000000  \n",
      "6  0.155747  0.000000  0.000000  0.000000  0.000000  0.155747  \n",
      "7  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "8  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "9  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "\n",
      "[10 rows x 140 columns]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=0.5, ngram_range=(1,1),\n",
    "                                 min_df=1,\n",
    "                                 )\n",
    "X = vectorizer.fit_transform(fear_df.Text)\n",
    "emotion_v = pd.DataFrame(\n",
    "    X.todense(),\n",
    "    columns=vectorizer.get_feature_names()\n",
    ")\n",
    "print(emotion_v)\n",
    "emotion_v.to_csv('emotion_v.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f140899a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>There are tons of other paintings that I thin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Yes .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Perhaps we need to have more babies ! Tina ga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Why ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Over there .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Me , too , I can ’ t stop scratching . They a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Why ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Why ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Make it 4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Jim !</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Emotion                                               Text\n",
       "0   neutral   There are tons of other paintings that I thin...\n",
       "8   neutral                                             Yes . \n",
       "14  neutral   Perhaps we need to have more babies ! Tina ga...\n",
       "15  neutral                                             Why ? \n",
       "23  neutral                                      Over there . \n",
       "25  neutral   Me , too , I can ’ t stop scratching . They a...\n",
       "28  neutral                                             Why ? \n",
       "50  neutral                                             Why ? \n",
       "54  neutral                                         Make it 4 \n",
       "66  neutral                                             Jim ! "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neutral_df = data[data['Emotion']=='neutral'].head(10)\n",
    "neutral_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f6e88294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        are  babies  baby    better  birth   boy       can  everywhere  gave  \\\n",
      "0  0.521880    0.00  0.00  0.306955   0.00  0.00  0.000000    0.000000  0.00   \n",
      "1  0.000000    0.00  0.00  0.000000   0.00  0.00  0.000000    0.000000  0.00   \n",
      "2  0.000000    0.25  0.25  0.000000   0.25  0.25  0.000000    0.000000  0.25   \n",
      "3  0.000000    0.00  0.00  0.000000   0.00  0.00  0.000000    0.000000  0.00   \n",
      "4  0.000000    0.00  0.00  0.000000   0.00  0.00  0.000000    0.000000  0.00   \n",
      "5  0.259606    0.00  0.00  0.000000   0.00  0.00  0.305386    0.305386  0.00   \n",
      "6  0.000000    0.00  0.00  0.000000   0.00  0.00  0.000000    0.000000  0.00   \n",
      "7  0.000000    0.00  0.00  0.000000   0.00  0.00  0.000000    0.000000  0.00   \n",
      "8  0.000000    0.00  0.00  0.000000   0.00  0.00  0.000000    0.000000  0.00   \n",
      "9  0.000000    0.00  0.00  0.000000   0.00  0.00  0.000000    0.000000  0.00   \n",
      "\n",
      "   have  ...      they     think  tina   to      tons       too    we  why  \\\n",
      "0  0.00  ...  0.000000  0.306955  0.00  0.0  0.306955  0.000000  0.00  0.0   \n",
      "1  0.00  ...  0.000000  0.000000  0.00  0.0  0.000000  0.000000  0.00  0.0   \n",
      "2  0.25  ...  0.000000  0.000000  0.25  0.5  0.000000  0.000000  0.25  0.0   \n",
      "3  0.00  ...  0.000000  0.000000  0.00  0.0  0.000000  0.000000  0.00  1.0   \n",
      "4  0.00  ...  0.000000  0.000000  0.00  0.0  0.000000  0.000000  0.00  0.0   \n",
      "5  0.00  ...  0.305386  0.000000  0.00  0.0  0.000000  0.305386  0.00  0.0   \n",
      "6  0.00  ...  0.000000  0.000000  0.00  0.0  0.000000  0.000000  0.00  1.0   \n",
      "7  0.00  ...  0.000000  0.000000  0.00  0.0  0.000000  0.000000  0.00  1.0   \n",
      "8  0.00  ...  0.000000  0.000000  0.00  0.0  0.000000  0.000000  0.00  0.0   \n",
      "9  0.00  ...  0.000000  0.000000  0.00  0.0  0.000000  0.000000  0.00  0.0   \n",
      "\n",
      "   yes  yesterday  \n",
      "0  0.0       0.00  \n",
      "1  1.0       0.00  \n",
      "2  0.0       0.25  \n",
      "3  0.0       0.00  \n",
      "4  0.0       0.00  \n",
      "5  0.0       0.00  \n",
      "6  0.0       0.00  \n",
      "7  0.0       0.00  \n",
      "8  0.0       0.00  \n",
      "9  0.0       0.00  \n",
      "\n",
      "[10 rows x 38 columns]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=0.5, ngram_range=(1,1),\n",
    "                                 min_df=1,\n",
    "                                 )\n",
    "X = vectorizer.fit_transform(neutral_df.Text)\n",
    "emotion_v1 = pd.DataFrame(\n",
    "    X.todense(),\n",
    "    columns=vectorizer.get_feature_names()\n",
    ")\n",
    "print(emotion_v1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
